{"cells":[{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["import os\n","import gc\n","import math\n","import pickle\n","from tqdm import tqdm\n","\n","import pandas as pd\n","import numpy as np\n","\n","import scipy.signal\n","from scipy.stats import beta\n","from scipy import stats\n","from sklearn.preprocessing import StandardScaler\n","from pykalman import KalmanFilter\n","\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, TensorDataset, Sampler\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning import LightningModule\n","from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","\n","PAST_DATA_PATH = 'D:/ubiquant_cache'\n","SUPPLE_DATA_FILE = 'D:/data/ubiquant/supplemental_train.csv'\n","PREPO_PATH = './preprocessing'\n","\n","\n","\n","def kalman_filter(series, tc):\n","\n","    kf = KalmanFilter(transition_matrices = [1],\n","    observation_matrices = [1],\n","    initial_state_mean = 0,\n","    initial_state_covariance = 1,\n","    observation_covariance = 1,\n","    transition_covariance = tc)\n","\n","    return kf.filter(series)[0].squeeze()\n","\n","def butterworth_filter(series, f=0.1):\n","\n","    if len(series) < 13:\n","        return series\n","\n","    b, a = scipy.signal.butter(3, f)\n","    return scipy.signal.filtfilt(b, a, series)\n","\n","def clip_corrs(corrs, thr_min=0.1, thr_max=0.4):\n","\n","    signs = np.sign(corrs)\n","    corrs = np.clip(np.abs(corrs),thr_min,thr_max)\n","    corrs = signs * (corrs - thr_min)\n","\n","    return corrs\n","\n","class PastResponseRegressor(LightningModule):\n","\n","    def __init__(self, input_width):\n","\n","        super(PastResponseRegressor, self).__init__()\n","\n","        hidden1 = input_width//2\n","        hidden2 = input_width//4\n","\n","        self.dense1 = nn.Linear(input_width, hidden1)\n","        self.dense2 = nn.Linear(hidden1, hidden2)\n","\n","        self.linear = nn.Linear(hidden2, 2)\n","\n","    def forward(self, x):\n","\n","        x = torch.relu(self.dense1(x))\n","        x = torch.relu(self.dense2(x))\n","\n","        return self.linear(x)\n","\n","class UbiquantDataSet(TensorDataset):\n","    \n","    def __init__(self, features, context, strat_corrs):\n","\n","        super(UbiquantDataSet, self).__init__()\n","\n","        self.features = features\n","        self.context = context\n","        self.strat_corrs = strat_corrs\n","\n","    def __len__(self):\n","\n","        return len(self.features)\n","\n","    def __getitem__(self, ndx):\n","\n","        return self.features[ndx], self.context[ndx], self.strat_corrs[ndx]\n","\n","    def __del__(self):\n","\n","        print('Dataset removed')\n","\n","class UbiquantSampler(Sampler):\n","\n","    def __init__(self, dataset, days_per_batch):\n","\n","        self.time_ids = dataset.context[:,0].detach().cpu().numpy()\n","        self.days_per_batch = days_per_batch\n","        self.num_batches = math.ceil(len(np.unique(self.time_ids)) / days_per_batch)\n","\n","    def __len__(self):\n","\n","        return self.num_batches\n","\n","    def __iter__(self):\n","\n","        unique_time_ids = np.unique(self.time_ids,return_counts=True)[0]\n","        rand_idx = np.random.permutation(unique_time_ids)\n","\n","        for i in np.arange(self.num_batches):\n","\n","            if i+1 == self.num_batches:\n","                random_days = rand_idx[self.days_per_batch*i:]\n","            else:\n","                random_days = rand_idx[self.days_per_batch*i:self.days_per_batch*(i+1)]\n","\n","            idx = np.where(np.isin(self.time_ids,random_days))[0]\n","\n","            np.random.shuffle(idx)\n","            yield  idx\n","\n","class UbiquantDataModule(pl.LightningDataModule):\n","\n","    def __init__(self, features, context, strat_corrs, split):\n","        \n","        super(UbiquantDataModule, self).__init__()\n","\n","        print('stacking past and supplemental data and copying to GPU...')\n","\n","        time_ids = context[:,0]\n","\n","        jump = int(split[-2])\n","        shift = int(split[-1])\n","\n","        block_size = torch.div(torch.max(time_ids)+1, 20, rounding_mode='floor')\n","        block_time_ids = torch.div(time_ids, block_size, rounding_mode='floor')\n","\n","        is_train = torch.remainder(block_time_ids + shift, jump) != 0\n","\n","        self.train_ds = UbiquantDataSet(features[is_train], context[is_train], strat_corrs[is_train])\n","        self.val_ds   = UbiquantDataSet(features[~is_train], context[~is_train], strat_corrs[~is_train])\n","\n","        print('train dataset size:', len(self.train_ds))\n","        print('val dataset size:', len(self.val_ds))\n","\n","    def train_dataloader(self):\n","\n","        return DataLoader(self.train_ds, shuffle=False, batch_sampler=UbiquantSampler(self.train_ds, 8))\n","    \n","    def val_dataloader(self):\n","\n","        return DataLoader(self.val_ds, shuffle=False, batch_sampler=UbiquantSampler(self.val_ds, 61))\n","\n","\n","###############################################################################################################\n","\n","\n","class UbiquantMultiTask(LightningModule):\n","\n","    def __init__(self, input_width):\n","\n","        super(UbiquantMultiTask, self).__init__()\n","\n","        hidden1 = int(input_width*1.2)\n","        hidden2 = int(input_width*0.6)\n","\n","        self.batch_norm1 = nn.BatchNorm1d(input_width)\n","        self.batch_norm2 = nn.BatchNorm1d(hidden1)\n","        self.batch_norm3 = nn.BatchNorm1d(hidden2)\n","\n","        self.dense1 = nn.Linear(input_width, hidden1)\n","        self.dense2 = nn.Linear(hidden1, hidden2)\n","        self.dense3 = nn.Linear(hidden2, hidden2)\n","\n","        self.dropout1 = torch.nn.Dropout(p=0.5)\n","        self.dropout2 = torch.nn.Dropout(p=0.5)\n","        self.dropout3 = torch.nn.Dropout(p=0.5)\n","\n","        self.linear = nn.Linear(hidden2, 6)\n","\n","    def forward(self, x):\n","\n","        x = self.batch_norm1(x)\n","        x = self.dense1(x)\n","        x = torch.relu(x)\n","        x = self.dropout1(x)\n","\n","        x = self.batch_norm2(x)\n","        x = self.dense2(x)\n","        x = torch.relu(x)\n","        x = self.dropout2(x)\n","\n","        x = self.batch_norm3(x)\n","        x = self.dense3(x)\n","        x = torch.relu(x)\n","        x = self.dropout3(x)\n","\n","        return self.linear(x)\n","\n","    def configure_optimizers(self):\n","\n","        optimizer = torch.optim.Adam(self.parameters(), lr=3e-3)\n","        sccheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=1, factor=1/3, verbose=True, min_lr=1e-3)\n","        return [optimizer], {'scheduler': sccheduler, 'monitor': 'val_corr_resp'}\n","\n","    def training_step(self, train_batch, batch_idx):\n","\n","        x, context, strat_corrs = train_batch\n","\n","        noisy_targets_responses = torch.column_stack((  torch.normal(mean=context[:,3],std=context[:,4]),\n","                                                        torch.normal(mean=context[:,5],std=context[:,6])))\n","\n","        noisy_targets_strat_corrs = torch.normal(mean=strat_corrs,std=1)\n","\n","        time_ids = context[:,0].reshape(-1,1)\n","\n","        x, noisy_targets_responses, noisy_targets_strat_corrs, time_ids = self.blend(   x, \n","                                                                                        noisy_targets_responses, \n","                                                                                        noisy_targets_strat_corrs, \n","                                                                                        time_ids, \n","                                                                                        ab=0.2)\n","\n","        noisy_targets_slow = noisy_targets_responses[:,0].reshape(-1,1)\n","        noisy_targets_fast = noisy_targets_responses[:,1].reshape(-1,1)\n","\n","        logits = self.forward(x)\n","        estimate_resp = logits[:,0].reshape(-1,1)\n","        estimate_slow = logits[:,1].reshape(-1,1)\n","        estimate_fast = logits[:,2].reshape(-1,1)\n","        estimate_strat_corrs = logits[:,3:]\n","\n","        noisy_targets_slow = torch.normal(mean=noisy_targets_slow,std=0.1*torch.abs(noisy_targets_slow))\n","        noisy_targets_fast = torch.normal(mean=noisy_targets_fast,std=0.1*torch.abs(noisy_targets_fast))\n","        noisy_targets_resp = torch.normal(mean=noisy_targets_slow+noisy_targets_fast,std=0.1*torch.abs(noisy_targets_slow+noisy_targets_fast))\n","        noisy_targets_strat_corrs = torch.normal(mean=noisy_targets_strat_corrs,std=1*torch.abs(noisy_targets_strat_corrs))\n","\n","        noisy_stratcorr_0 = 1.*(noisy_targets_strat_corrs[:,0]>0)\n","        stratcorr_0_ce = F.binary_cross_entropy_with_logits(estimate_strat_corrs[:,0], noisy_stratcorr_0)\n","\n","        noisy_stratcorr_1 = 1.*(noisy_targets_strat_corrs[:,1]>0)\n","        stratcorr_1_ce = F.binary_cross_entropy_with_logits(estimate_strat_corrs[:,1], noisy_stratcorr_1)\n","\n","        noisy_stratcorr_2 = 1.*(noisy_targets_strat_corrs[:,2]>0)\n","        stratcorr_2_ce = F.binary_cross_entropy_with_logits(estimate_strat_corrs[:,2], noisy_stratcorr_2)\n","\n","        noisy_corr_slow_loss, _ = self.day_metric_mean(estimate_slow, noisy_targets_slow, time_ids, clip_value=0.55, is_loss=True)\n","        noisy_corr_fast_loss, _ = self.day_metric_mean(estimate_fast, noisy_targets_fast, time_ids, clip_value=0.45, is_loss=True)\n","        noisy_corr_resp_loss, _ = self.day_metric_mean(estimate_resp, noisy_targets_resp, time_ids, clip_value=0.3, is_loss=True)\n","\n","        loss_resp = noisy_corr_resp_loss + noisy_corr_slow_loss + noisy_corr_fast_loss\n","        loss_corr = stratcorr_0_ce + stratcorr_1_ce + stratcorr_2_ce\n","\n","        return loss_resp + loss_corr\n","\n","    def validation_step(self, val_batch, batch_idx):\n","\n","        x, context, _ = val_batch\n","        \n","        estimate_resp = self(x)[:,0].reshape(-1,1)\n","        targets_resp = (context[:,3] + context[:,5]).reshape(-1,1)\n","        time_ids = context[:,0].reshape(-1,1)\n","\n","        self.log('val_corr_resp', self.day_metric_mean(estimate_resp, targets_resp, time_ids)[0].item())\n","\n","    def blend(self, feat, resp, corr, tids, ab=0.4):\n","\n","        blended = [], [], [], []\n","\n","        if len(feat) < 2:\n","            blended[0].append(feat)\n","            blended[1].append(resp)\n","            blended[2].append(corr)\n","            blended[3].append(tids)\n","        else:\n","\n","            if len(feat) % 2 > 0:\n","                feat = feat[:-1]\n","                resp = resp[:-1]\n","                corr = corr[:-1]\n","                tids = tids[:-1]\n","\n","            b = torch.tensor(beta.rvs(ab, ab, size=len(feat)//2), device='cuda', dtype=torch.float32).reshape(-1,1)\n","\n","            blended[0].append(b * feat[::2] + (1-b) * feat[1::2])\n","            blended[1].append(b * resp[::2] + (1-b) * resp[1::2])\n","            blended[2].append(b * corr[::2] + (1-b) * corr[1::2])\n","            blended[3].append( torch.where(b > 0.5, tids[::2], tids[1::2]) )\n","\n","        return torch.vstack(blended[0]), torch.vstack(blended[1]), torch.vstack(blended[2]), torch.vstack(blended[3])\n","\n","    def day_metric_mean(self, estimates, targets, day_ids, clip_value=1, is_loss=False, method='pearson'):\n","\n","        day_metrics = []\n","        day_weights = []\n","\n","        for day_id in torch.unique(day_ids):\n","\n","            idx = day_ids == day_id\n","\n","            e = estimates[idx]\n","            t = targets[idx]\n","\n","            if len(e) < 2:\n","                continue\n","\n","            try:\n","                if is_loss:\n","\n","                    if method=='pearson':\n","                        day_metric = -self.pearson_corr(e,t)\n","                        day_metric = torch.clip(day_metric, -clip_value, 1)\n","                    elif method=='mse':\n","                        day_metric = F.mse_loss(e, t)\n","                    else:\n","                        print('unknown loss function')\n","                else:\n","                    if method=='pearson':\n","                        day_metric = self.pearson_corr(e,t)\n","                    elif method=='mse':\n","                        day_metric = F.l1_loss(e, t)\n","                    else:\n","                        print('unknown loss function')\n","            except Exception as ex:\n","                print('day_metric_mean error:',ex)\n","                continue\n","\n","            day_metrics.append(day_metric)\n","\n","            length = torch.tensor(len(e), dtype=torch.float32, device=torch.device('cuda'), requires_grad=is_loss)\n","            day_weights.append(length)\n","\n","        if len(day_metrics) == 0:\n","\n","            return torch.tensor(0, device='cuda'), torch.tensor(0, device='cuda')\n","\n","        day_metrics = torch.stack(day_metrics)\n","        day_weights = torch.stack(day_weights)\n","\n","        return torch.sum(day_metrics * day_weights) / torch.sum(day_weights), torch.min(day_metrics)\n","\n","    def pearson_corr(self, x, y):\n","\n","        cos = nn.CosineSimilarity(dim=0, eps=1e-6)\n","        return cos(x - x.mean(dim=0, keepdim=True), y - y.mean(dim=0, keepdim=True))"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["reading supplemental_train...\n","computing series...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4098/4098 [04:31<00:00, 15.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["scaling...\n","PCA...\n","loading past response regressor...\n","computing past responses estimates...\n","computing day means and correlations to estimated past targets...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/3638 [00:00<?, ?it/s]C:\\Users\\codef\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2642: RuntimeWarning: invalid value encountered in true_divide\n","  c /= stddev[:, None]\n","C:\\Users\\codef\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2643: RuntimeWarning: invalid value encountered in true_divide\n","  c /= stddev[None, :]\n","  0%|          | 4/3638 [00:00<01:35, 38.17it/s]C:\\Users\\codef\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py:4484: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n","  warnings.warn(SpearmanRConstantInputWarning())\n"," 99%|█████████▉| 3617/3638 [01:15<00:00, 80.48it/s]C:\\Users\\codef\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2634: RuntimeWarning: Degrees of freedom <= 0 for slice\n","  c = cov(x, y, rowvar, dtype=dtype)\n","C:\\Users\\codef\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2493: RuntimeWarning: divide by zero encountered in true_divide\n","  c *= np.true_divide(1, fact)\n","C:\\Users\\codef\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2493: RuntimeWarning: invalid value encountered in multiply\n","  c *= np.true_divide(1, fact)\n","100%|██████████| 3638/3638 [01:15<00:00, 47.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["computing day and corr PCAs...\n","extending features with day features...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 3638/3638 [00:13<00:00, 279.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["computing strat correlations to target...\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/4098 [00:00<?, ?it/s]C:\\Users\\codef\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2642: RuntimeWarning: invalid value encountered in true_divide\n","  c /= stddev[:, None]\n","C:\\Users\\codef\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:2643: RuntimeWarning: invalid value encountered in true_divide\n","  c /= stddev[None, :]\n","100%|██████████| 4098/4098 [00:23<00:00, 175.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["creating strat_corrs array...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4098/4098 [00:13<00:00, 300.16it/s]\n"]},{"data":{"text/plain":["9"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["torch.manual_seed(0)\n","np.random.seed(0)\n","\n","print('reading supplemental_train...')\n","original_features = pd.read_csv(SUPPLE_DATA_FILE, dtype=np.float32, usecols=np.array(range(300))+3).values\n","original_context  = pd.read_csv(SUPPLE_DATA_FILE, dtype=np.float32, usecols=[1,2,303]).values\n","gc.collect()\n","\n","context = np.empty((len(original_context),11), dtype=np.float32)\n","context[:] = np.nan\n","context[:,:3] = original_context\n","\n","# to avoid clashing with past data\n","context[:,0] = context[:,0] + 1220\n","context[:,1] = context[:,1] + 3774\n","\n","time_ids  = context[:,0]\n","strat_ids = context[:,1]\n","targets = context[:,2]\n","\n","print('computing series...')\n","for strat_id in tqdm(np.unique(strat_ids)):\n","\n","    idx = strat_ids==strat_id\n","    strat_targets = targets[idx]\n","\n","    context[idx,3] = np.diff(kalman_filter(strat_targets.cumsum(),0.2),prepend=0)  # slow signal\n","    context[idx,4] = pd.Series(context[idx,3]).rolling(window=5, min_periods=1).std().to_numpy(dtype=np.float32)  # slow signal rolling std\n","\n","    context[idx,5] = strat_targets - context[idx,3]  # fast signal\n","    context[idx,6] = pd.Series(context[idx,5]).rolling(window=5, min_periods=1).std().to_numpy(dtype=np.float32)  # fast signal rolling std\n","\n","    context[idx,7] = np.roll(np.diff(butterworth_filter(strat_targets.cumsum(),0.2),prepend=0),1); context[0,7] = 0  # filtered signal lagged\n","    context[idx,8] = pd.Series(context[idx,7]).rolling(window=5, min_periods=1).std().to_numpy(dtype=np.float32)  # filtered signal lagged rolling std\n","\n","    context[idx,9] = pd.Series(context[idx,7]).rolling(window=5, min_periods=1).mean().to_numpy(dtype=np.float32)  # filtered signal lagged rolling mean\n","    context[idx,10] = pd.Series(context[idx,9]).rolling(window=5, min_periods=1).std().to_numpy(dtype=np.float32)  # filtered signal lagged rolling mean rolling std\n","\n","context = np.nan_to_num(context)\n","\n","del original_context\n","gc.collect()\n","\n","targets_slow = context[:,3]\n","targets_past = context[:,[7,9]]\n","\n","print('scaling...')\n","scaler = pickle.load(open(PREPO_PATH+'/scaler_EVAL.pkl','rb'))\n","features = scaler.transform(original_features)\n","\n","print('PCA...')\n","pca_fea = pickle.load(open(PREPO_PATH+'/pca_fea_EVAL.pkl','rb'))\n","features = pca_fea.transform(features).astype(np.float32)\n","\n","print('loading past response regressor...')\n","regressor_path = PREPO_PATH+'/regressor_EVAL.ckpt'\n","regressor = PastResponseRegressor.load_from_checkpoint(regressor_path,input_width=features.shape[1])\n","\n","print('computing past responses estimates...')\n","f = torch.from_numpy(features).to('cuda')\n","regressor.eval()\n","regressor.cuda()\n","estimate_targets_past = regressor(f).detach().cpu().numpy()\n","\n","regressor.cpu()\n","del regressor\n","del f\n","\n","degra = 3\n","red_num_pcs = 81\n","max_time_id = int(max(time_ids))+1\n","features_de = np.clip(np.round(features[:,:red_num_pcs] * degra), -2*degra, 2*degra)\n","\n","time_index = np.array(range(max_time_id))\n","day_means = np.empty((max_time_id,original_features.shape[1]), dtype=np.float32);  day_means[:] = np.nan\n","pears_to_target = np.empty((max_time_id,(red_num_pcs+2)*2), dtype=np.float32);  pears_to_target[:] = np.nan\n","spear_to_target = np.empty((max_time_id,(red_num_pcs+2)*2), dtype=np.float32);  spear_to_target[:] = np.nan\n","\n","print('computing day means and correlations to estimated past targets...')\n","for time_id in tqdm(np.unique(time_ids)):\n","\n","    day_means[int(time_id)] = np.nan_to_num(np.mean(original_features[time_ids==time_id],axis=0), posinf=0., neginf=0.)\n","\n","    features_daygroup_de  = features_de[time_ids==time_id]\n","    targs_daygroup_de = 1.*(estimate_targets_past[time_ids==time_id]>0)\n","\n","    try:\n","        pears = np.corrcoef(np.hstack((targs_daygroup_de,features_daygroup_de)).T)[:,:2]\n","        pears_to_target[int(time_id)] = np.nan_to_num(pears, posinf=0., neginf=0.).flatten()\n","    except Exception as ex:\n","        pears_to_target[int(time_id)] = np.zeros((red_num_pcs+2)*2, dtype=np.float32)\n","\n","    try:\n","        spear = stats.spearmanr(np.hstack((targs_daygroup_de,features_daygroup_de)))[0][:,:2].astype(np.float32)\n","        spear_to_target[int(time_id)] = np.nan_to_num(spear, posinf=0., neginf=0.).flatten()\n","    except Exception as ex:\n","        spear_to_target[int(time_id)] = np.zeros((red_num_pcs+2)*2, dtype=np.float32)\n","\n","del original_features\n","gc.collect()\n","\n","idx = ~np.isnan(day_means).any(axis=1)\n","time_index = time_index[idx]\n","day_means = day_means[idx]\n","pears_to_target = pears_to_target[idx]\n","spear_to_target = spear_to_target[idx]\n","\n","pears_to_target = clip_corrs(pears_to_target)\n","spear_to_target = pears_to_target - clip_corrs(spear_to_target)\n","\n","pca_day   = pickle.load(open(PREPO_PATH+'/pca_day_EVAL.pkl','rb'))\n","pca_pears = pickle.load(open(PREPO_PATH+'/pca_pears_EVAL.pkl','rb'))\n","pca_spear = pickle.load(open(PREPO_PATH+'/pca_spear_EVAL.pkl','rb'))\n","\n","print('computing day and corr PCAs...')\n","day_means = pca_day.transform(day_means).astype(np.float32)\n","pears_to_target = pca_pears.transform(pears_to_target).astype(np.float32)\n","spear_to_target = pca_spear.transform(spear_to_target).astype(np.float32)\n","\n","daily_feats = np.hstack(( day_means, pears_to_target, spear_to_target ))\n","features_ext = np.empty((len(features),daily_feats.shape[1]), dtype=np.float32);  features_ext[:] = np.nan\n","\n","print('extending features with day features...')\n","for i in tqdm(range(len(time_index))):\n","\n","    features_ext[time_ids==time_index[i]] = daily_feats[i]\n","\n","features = np.hstack(( features,features_ext,estimate_targets_past ))\n","\n","degra = 3\n","red_num_pcs = 3\n","features_de = np.clip(np.round(features[:,:red_num_pcs] * degra), -2*degra, 2*degra)\n","\n","daily_strat_corrs = np.empty((int(max(strat_ids))+1,red_num_pcs), dtype=np.float32); daily_strat_corrs[:] = np.nan\n","daily_strat_index = np.array(range(len(daily_strat_corrs)))\n","\n","print('computing strat correlations to target...')\n","for strat_id in tqdm(np.unique(strat_ids)):\n","\n","    feats_strat_de = features_de[strat_ids==strat_id]\n","    targs_stratgroup_de = 1.*(targets_slow[strat_ids==strat_id]>0)\n","\n","    try:\n","        corrs = np.corrcoef(np.hstack((targs_stratgroup_de.reshape(-1,1),feats_strat_de)).T)[1:,0]\n","        corrs = np.nan_to_num(corrs, posinf=0., neginf=0.)\n","    except Exception as ex:\n","        corrs = np.zeros(red_num_pcs, dtype=np.float32)\n","\n","    daily_strat_corrs[int(strat_id)] = corrs\n","    daily_strat_index[int(strat_id)] = strat_id\n","\n","idx = ~np.isnan(daily_strat_corrs).any(axis=1)\n","daily_strat_corrs = daily_strat_corrs[idx]\n","daily_strat_index = daily_strat_index[idx]\n","\n","signs = np.sign(daily_strat_corrs)\n","daily_strat_corrs = np.clip(np.abs(daily_strat_corrs),0.1,0.3)\n","daily_strat_corrs = signs * (daily_strat_corrs - 0.1)\n","daily_strat_corrs = StandardScaler().fit_transform(daily_strat_corrs)\n","\n","strat_corrs = np.empty((len(features),daily_strat_corrs.shape[1]), dtype=np.float32);  strat_corrs[:] = np.nan\n","\n","print('creating strat_corrs array...')\n","for i in tqdm(range(len(daily_strat_index))):\n","\n","    strat_corrs[strat_ids==daily_strat_index[i]] = daily_strat_corrs[i]\n","\n","del features_de,day_means,pears_to_target,spear_to_target,daily_feats,daily_strat_corrs,daily_strat_index,time_index,idx,signs\n","gc.collect()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["stacking past and supplemental data and copying to GPU...\n"]},{"data":{"text/plain":["0"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["print('stacking past and supplemental data and copying to GPU...')\n","\n","with open(PAST_DATA_PATH+'/context.pkl','rb') as fc:\n","    \n","    context  = torch.tensor(np.vstack((pickle.load(fc),context)), dtype=torch.float32, device='cuda')\n","\n","with open(PAST_DATA_PATH+'/strat_corrs_EVAL.pkl','rb') as fs:\n","\n","    strat_corrs = torch.tensor(np.vstack((pickle.load(fs),strat_corrs)), dtype=torch.float32, device='cuda')\n","\n","gc.collect()\n","\n","with open(PAST_DATA_PATH+'/features_EVAL.pkl','rb') as ff:\n","    \n","    features = torch.tensor(np.vstack((pickle.load(ff),features)), dtype=torch.float32, device='cuda')\n","\n","gc.collect()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["training models...\n","\n","training model for split CV50\n","loading data\n","stacking past and supplemental data and copying to GPU...\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name        | Type        | Params\n","--------------------------------------------\n","0 | batch_norm1 | BatchNorm1d | 500   \n","1 | batch_norm2 | BatchNorm1d | 600   \n","2 | batch_norm3 | BatchNorm1d | 300   \n","3 | dense1      | Linear      | 75.3 K\n","4 | dense2      | Linear      | 45.1 K\n","5 | dense3      | Linear      | 22.7 K\n","6 | dropout1    | Dropout     | 0     \n","7 | dropout2    | Dropout     | 0     \n","8 | dropout3    | Dropout     | 0     \n","9 | linear      | Linear      | 906   \n","--------------------------------------------\n","145 K     Trainable params\n","0         Non-trainable params\n","145 K     Total params\n","0.582     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["train dataset size: 3309165\n","val dataset size: 775206\n","creating model instance\n","go!\n","Epoch 0: 100%|██████████| 500/500 [01:06<00:00,  7.52it/s, loss=1.49] "]},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 482: val_corr_resp reached 0.08519 (best 0.08519), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV50.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: 100%|██████████| 500/500 [01:06<00:00,  7.51it/s, loss=1.44]"]},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 965: val_corr_resp reached 0.08835 (best 0.08835), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV50.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: 100%|██████████| 500/500 [01:03<00:00,  7.91it/s, loss=1.52]"]},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 1448: val_corr_resp reached 0.09819 (best 0.09819), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV50.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: 100%|██████████| 500/500 [01:06<00:00,  7.55it/s, loss=1.45]"]},{"name":"stderr","output_type":"stream","text":["Epoch 3, step 1931: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: 100%|█████████▉| 498/500 [01:05<00:00,  7.62it/s, loss=1.51]Epoch     5: reducing learning rate of group 0 to 1.0000e-03.\n","Epoch 4: 100%|██████████| 500/500 [01:06<00:00,  7.55it/s, loss=1.51]"]},{"name":"stderr","output_type":"stream","text":["Epoch 4, step 2414: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: 100%|██████████| 500/500 [01:06<00:00,  7.55it/s, loss=1.51]\n","Dataset removed\n","Dataset removed\n","\n","training model for split CV51\n","loading data\n","stacking past and supplemental data and copying to GPU...\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\codef\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:68: UserWarning: Checkpoint directory ./models exists and is not empty.\n","  warnings.warn(*args, **kwargs)\n","GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name        | Type        | Params\n","--------------------------------------------\n","0 | batch_norm1 | BatchNorm1d | 500   \n","1 | batch_norm2 | BatchNorm1d | 600   \n","2 | batch_norm3 | BatchNorm1d | 300   \n","3 | dense1      | Linear      | 75.3 K\n","4 | dense2      | Linear      | 45.1 K\n","5 | dense3      | Linear      | 22.7 K\n","6 | dropout1    | Dropout     | 0     \n","7 | dropout2    | Dropout     | 0     \n","8 | dropout3    | Dropout     | 0     \n","9 | linear      | Linear      | 906   \n","--------------------------------------------\n","145 K     Trainable params\n","0         Non-trainable params\n","145 K     Total params\n","0.582     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["train dataset size: 3158641\n","val dataset size: 925730\n","creating model instance\n","go!\n","Epoch 0: 100%|██████████| 502/502 [01:05<00:00,  7.72it/s, loss=1.48] "]},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 485: val_corr_resp reached 0.09577 (best 0.09577), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV51.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: 100%|██████████| 502/502 [01:04<00:00,  7.78it/s, loss=1.45]"]},{"name":"stderr","output_type":"stream","text":["Epoch 1, step 971: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: 100%|█████████▉| 501/502 [01:02<00:00,  8.07it/s, loss=1.54]Epoch     3: reducing learning rate of group 0 to 1.0000e-03.\n","Epoch 2: 100%|██████████| 502/502 [01:02<00:00,  7.97it/s, loss=1.54]"]},{"name":"stderr","output_type":"stream","text":["Epoch 2, step 1457: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: 100%|██████████| 502/502 [01:02<00:00,  7.97it/s, loss=1.54]\n","Dataset removed\n","Dataset removed\n","\n","training model for split CV52\n","loading data\n","stacking past and supplemental data and copying to GPU...\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name        | Type        | Params\n","--------------------------------------------\n","0 | batch_norm1 | BatchNorm1d | 500   \n","1 | batch_norm2 | BatchNorm1d | 600   \n","2 | batch_norm3 | BatchNorm1d | 300   \n","3 | dense1      | Linear      | 75.3 K\n","4 | dense2      | Linear      | 45.1 K\n","5 | dense3      | Linear      | 22.7 K\n","6 | dropout1    | Dropout     | 0     \n","7 | dropout2    | Dropout     | 0     \n","8 | dropout3    | Dropout     | 0     \n","9 | linear      | Linear      | 906   \n","--------------------------------------------\n","145 K     Trainable params\n","0         Non-trainable params\n","145 K     Total params\n","0.582     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["train dataset size: 3182246\n","val dataset size: 902125\n","creating model instance\n","go!\n","Epoch 0: 100%|██████████| 502/502 [01:04<00:00,  7.77it/s, loss=1.43] "]},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 485: val_corr_resp reached 0.07020 (best 0.07020), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV52.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: 100%|██████████| 502/502 [01:04<00:00,  7.74it/s, loss=1.56]"]},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 971: val_corr_resp reached 0.08896 (best 0.08896), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV52.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: 100%|██████████| 502/502 [01:03<00:00,  7.93it/s, loss=1.47]"]},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 1457: val_corr_resp reached 0.09719 (best 0.09719), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV52.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: 100%|██████████| 502/502 [01:02<00:00,  8.00it/s, loss=1.48]"]},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 1943: val_corr_resp reached 0.10005 (best 0.10005), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV52.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: 100%|██████████| 502/502 [01:02<00:00,  7.98it/s, loss=1.45]"]},{"name":"stderr","output_type":"stream","text":["Epoch 4, step 2429: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: 100%|██████████| 502/502 [01:03<00:00,  7.93it/s, loss=1.47]"]},{"name":"stderr","output_type":"stream","text":["Epoch 5, global step 2915: val_corr_resp reached 0.10119 (best 0.10119), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV52.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6: 100%|██████████| 502/502 [01:03<00:00,  7.93it/s, loss=1.55]"]},{"name":"stderr","output_type":"stream","text":["Epoch 6, global step 3401: val_corr_resp reached 0.10673 (best 0.10673), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV52.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 7: 100%|██████████| 502/502 [01:03<00:00,  7.94it/s, loss=1.47]"]},{"name":"stderr","output_type":"stream","text":["Epoch 7, step 3887: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8: 100%|█████████▉| 501/502 [01:01<00:00,  8.11it/s, loss=1.49]Epoch     9: reducing learning rate of group 0 to 1.0000e-03.\n","Epoch 8: 100%|██████████| 502/502 [01:02<00:00,  8.02it/s, loss=1.49]"]},{"name":"stderr","output_type":"stream","text":["Epoch 8, step 4373: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 8: 100%|██████████| 502/502 [01:02<00:00,  8.02it/s, loss=1.49]\n","Dataset removed\n","Dataset removed\n","\n","training model for split CV53\n","loading data\n","stacking past and supplemental data and copying to GPU...\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name        | Type        | Params\n","--------------------------------------------\n","0 | batch_norm1 | BatchNorm1d | 500   \n","1 | batch_norm2 | BatchNorm1d | 600   \n","2 | batch_norm3 | BatchNorm1d | 300   \n","3 | dense1      | Linear      | 75.3 K\n","4 | dense2      | Linear      | 45.1 K\n","5 | dense3      | Linear      | 22.7 K\n","6 | dropout1    | Dropout     | 0     \n","7 | dropout2    | Dropout     | 0     \n","8 | dropout3    | Dropout     | 0     \n","9 | linear      | Linear      | 906   \n","--------------------------------------------\n","145 K     Trainable params\n","0         Non-trainable params\n","145 K     Total params\n","0.582     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["train dataset size: 3285340\n","val dataset size: 799031\n","creating model instance\n","go!\n","Epoch 0: 100%|██████████| 502/502 [01:02<00:00,  8.02it/s, loss=1.51] "]},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 485: val_corr_resp reached 0.10088 (best 0.10088), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV53.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: 100%|██████████| 502/502 [01:04<00:00,  7.79it/s, loss=1.52]"]},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 971: val_corr_resp reached 0.10693 (best 0.10693), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV53.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: 100%|██████████| 502/502 [01:03<00:00,  7.88it/s, loss=1.42]"]},{"name":"stderr","output_type":"stream","text":["Epoch 2, global step 1457: val_corr_resp reached 0.10873 (best 0.10873), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV53.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: 100%|██████████| 502/502 [01:03<00:00,  7.95it/s, loss=1.44]"]},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 1943: val_corr_resp reached 0.11141 (best 0.11141), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV53.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: 100%|██████████| 502/502 [01:02<00:00,  7.98it/s, loss=1.56]"]},{"name":"stderr","output_type":"stream","text":["Epoch 4, global step 2429: val_corr_resp reached 0.11335 (best 0.11335), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV53.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: 100%|██████████| 502/502 [01:06<00:00,  7.59it/s, loss=1.43]"]},{"name":"stderr","output_type":"stream","text":["Epoch 5, step 2915: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6: 100%|█████████▉| 500/502 [01:04<00:00,  7.76it/s, loss=1.41]Epoch     7: reducing learning rate of group 0 to 1.0000e-03.\n","Epoch 6: 100%|██████████| 502/502 [01:05<00:00,  7.62it/s, loss=1.41]"]},{"name":"stderr","output_type":"stream","text":["Epoch 6, step 3401: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6: 100%|██████████| 502/502 [01:05<00:00,  7.62it/s, loss=1.41]\n","Dataset removed\n","Dataset removed\n","\n","training model for split CV54\n","loading data\n","stacking past and supplemental data and copying to GPU...\n"]},{"name":"stderr","output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name        | Type        | Params\n","--------------------------------------------\n","0 | batch_norm1 | BatchNorm1d | 500   \n","1 | batch_norm2 | BatchNorm1d | 600   \n","2 | batch_norm3 | BatchNorm1d | 300   \n","3 | dense1      | Linear      | 75.3 K\n","4 | dense2      | Linear      | 45.1 K\n","5 | dense3      | Linear      | 22.7 K\n","6 | dropout1    | Dropout     | 0     \n","7 | dropout2    | Dropout     | 0     \n","8 | dropout3    | Dropout     | 0     \n","9 | linear      | Linear      | 906   \n","--------------------------------------------\n","145 K     Trainable params\n","0         Non-trainable params\n","145 K     Total params\n","0.582     Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["train dataset size: 3402092\n","val dataset size: 682279\n","creating model instance\n","go!\n","Epoch 0: 100%|██████████| 503/503 [01:06<00:00,  7.58it/s, loss=1.54] "]},{"name":"stderr","output_type":"stream","text":["Epoch 0, global step 486: val_corr_resp reached 0.10340 (best 0.10340), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV54.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: 100%|██████████| 503/503 [01:06<00:00,  7.55it/s, loss=1.5] "]},{"name":"stderr","output_type":"stream","text":["Epoch 1, global step 973: val_corr_resp reached 0.11135 (best 0.11135), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV54.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 2: 100%|██████████| 503/503 [01:06<00:00,  7.53it/s, loss=1.51]"]},{"name":"stderr","output_type":"stream","text":["Epoch 2, step 1460: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: 100%|██████████| 503/503 [01:07<00:00,  7.50it/s, loss=1.52]"]},{"name":"stderr","output_type":"stream","text":["Epoch 3, global step 1947: val_corr_resp reached 0.11533 (best 0.11533), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV54.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 4: 100%|██████████| 503/503 [01:06<00:00,  7.54it/s, loss=1.52]"]},{"name":"stderr","output_type":"stream","text":["Epoch 4, step 2434: val_corr_resp was not in top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 5: 100%|██████████| 503/503 [01:06<00:00,  7.51it/s, loss=1.59]"]},{"name":"stderr","output_type":"stream","text":["Epoch 5, global step 2921: val_corr_resp reached 0.11565 (best 0.11565), saving model to \"C:\\Users\\codef\\OneDrive\\machine learning\\ubiquant\\models\\multitask_CV54.ckpt\" as top 1\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 6:  50%|█████     | 254/503 [00:32<00:31,  7.93it/s, loss=1.46]"]}],"source":["print('training models...')\n","\n","for split in np.arange(5):\n","\n","    file_name = './models/multitask_CV5{}.ckpt'.format(split)\n","\n","    if os.path.isfile(file_name):\n","        os.remove(file_name)\n","\n","torch.manual_seed(0)\n","np.random.seed(0)\n","\n","for split in np.arange(5):\n","\n","    split_name = 'CV5{}'.format(split)\n","\n","    print()\n","    print('training model for split', split_name)\n","    \n","    print('loading data')\n","    data = UbiquantDataModule(features, context, strat_corrs, split_name)\n","\n","    print('creating model instance')\n","    model = UbiquantMultiTask(input_width=250)\n","\n","    monitor='val_corr_resp'\n","    mode='max'\n","\n","    early_stop_callback = EarlyStopping(\n","        monitor=monitor,\n","        mode=mode,\n","        patience=2,\n","        min_delta=0.0001,\n","        verbose=True\n","    )\n","\n","    checkpoint_callback = ModelCheckpoint(\n","        monitor=monitor,\n","        mode=mode,\n","        dirpath='./models',\n","        filename='multitask_{}'.format(split_name),\n","        save_top_k=1,\n","        verbose=True\n","    )\n","\n","    trainer = pl.Trainer(   logger=False,\n","                            gpus=1,\n","                            max_epochs=18,\n","                            checkpoint_callback=True,\n","                            callbacks=[early_stop_callback,checkpoint_callback]\n","                            )\n","\n","    print('go!')\n","    trainer.fit(model, data)\n","\n","    model.cpu()\n","\n","    for optimizer_metrics in trainer.optimizers[0].state.values():\n","        for metric_name, metric in optimizer_metrics.items():\n","            if torch.is_tensor(metric):\n","                optimizer_metrics[metric_name] = metric.cpu()\n","\n","    del early_stop_callback, checkpoint_callback, trainer, model, data\n","\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('loading models and prepro...')\n","\n","scalers = []\n","regressors = []\n","models = []\n","\n","compressors_fea = []\n","compressors_day = []\n","compressors_pea = []\n","compressors_spe = []\n","\n","for split in np.arange(5):\n","\n","    print('..for split',split)\n","\n","    scalers.append(pickle.load(open(PREPO_PATH+'/scaler_CV5{}.pkl'.format(split),'rb')))\n","\n","    compressors_fea.append(pickle.load(open(PREPO_PATH+'/pca_fea_CV5{}.pkl'.format(split),'rb')))\n","    compressors_day.append(pickle.load(open(PREPO_PATH+'/pca_day_CV5{}.pkl'.format(split),'rb')))\n","    compressors_pea.append(pickle.load(open(PREPO_PATH+'/pca_pears_CV5{}.pkl'.format(split),'rb')))\n","    compressors_spe.append(pickle.load(open(PREPO_PATH+'/pca_spear_CV5{}.pkl'.format(split),'rb')))\n","\n","    regressor = PastResponseRegressor.load_from_checkpoint(PREPO_PATH+'/regressor_CV5{}.ckpt'.format(split), input_width=210)\n","    regressor.cpu()\n","    regressor.eval()\n","    regressors.append(regressor)\n","\n","    model = UbiquantMultiTask.load_from_checkpoint('./models/multitask_CV5{}.ckpt'.format(split), input_width=250)\n","    model.cpu()\n","    model.eval()\n","    models.append(model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["iter_test = [pd.read_csv('D:/data/ubiquant/example_test.csv').drop(columns=['time_id'])]\n","submission_df = pd.DataFrame(columns=['row_id,target'])\n","# USE BELOW CODE INSTEAD FOR KAGGLE COMPETITION\n","# import ubiquant\n","# env = ubiquant.make_env()\n","# iter_test = env.iter_test()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["degra = 3\n","red_num_pcs = 81\n","\n","print('inference starts...')\n","\n","for test_df in iter_test:\n","# USE BELOW CODE INSTEAD FOR KAGGLE COMPETITION\n","# for (test_df, sample_prediction_df) in iter_test:\n","    \n","    if len(test_df) == 0:\n","        continue\n","\n","    original_features = test_df.iloc[:,2:].to_numpy(dtype=np.float32)\n","\n","    day_means = np.mean(original_features,axis=0).reshape(1,-1)\n","\n","    output = np.zeros(len(test_df))\n","\n","    for split in np.arange(5):\n","        \n","        try:\n","        \n","            features = scalers[split].transform(original_features)\n","            features = compressors_fea[split].transform(features).astype(np.float32)\n","\n","            try:\n","                f = torch.from_numpy(features)\n","                estimate_targets_past = regressors[split](f).detach().cpu().numpy()\n","                estimate_targets_past = np.nan_to_num(estimate_targets_past, posinf=0., neginf=0.)\n","            except Exception as ex:\n","                print('PastResponseRegressor error:',ex)\n","                estimate_targets_past = np.zeros((len(features),2), dtype=np.float32)\n","\n","            features_daygroup_de = np.clip(np.round(features[:,:red_num_pcs] * degra), -2*degra, 2*degra)\n","            targs_daygroup_de = 1.*(estimate_targets_past>0)\n","\n","            try:\n","                pears = np.corrcoef(np.hstack((targs_daygroup_de,features_daygroup_de)).T)[:,:2]\n","                pears = np.nan_to_num(pears, posinf=0., neginf=0.).flatten().reshape(1,-1)\n","            except Exception as ex:\n","                pears = np.zeros((1,(red_num_pcs+2)*2), dtype=np.float32)\n","\n","            try:\n","                spear = stats.spearmanr(np.hstack((targs_daygroup_de,features_daygroup_de)))[0][:,:2].astype(np.float32)\n","                spear = np.nan_to_num(spear, posinf=0., neginf=0.).flatten().reshape(1,-1)\n","            except Exception as ex:\n","                spear = np.zeros((1,(red_num_pcs+2)*2), dtype=np.float32)\n","\n","            pears = clip_corrs(pears)\n","            spear = pears - clip_corrs(spear)\n","\n","            day_means_split = compressors_day[split].transform(day_means).astype(np.float32)\n","            pears = compressors_pea[split].transform(pears).astype(np.float32)\n","            spear = compressors_spe[split].transform(spear).astype(np.float32)\n","\n","            features = np.nan_to_num(np.hstack((features,\n","                                      np.repeat(day_means_split,len(features),axis=0),\n","                                      np.repeat(pears,len(features),axis=0),\n","                                      np.repeat(spear,len(features),axis=0),\n","                                      estimate_targets_past)), posinf=0., neginf=0.)\n","\n","            model_feats = torch.from_numpy(features)\n","            model_output = models[split](model_feats).detach().numpy()\n","            model_output = np.nan_to_num(model_output, posinf=0., neginf=0.)\n","            output += model_output[:,0] / 5\n","            \n","        except Exception as ex:\n","            print('Iteration error:',ex)\n","\n","    col_id = pd.Series(test_df.iloc[:,0])\n","    col_target = pd.Series(output)\n","    submission_df = pd.DataFrame({ 'row_id':col_id, 'target':col_target })\n","    # USE BELOW CODE INSTEAD FOR KAGGLE COMPETITION\n","    # sample_prediction_df['target'] = output\n","    # env.predict(sample_prediction_df)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["347\n"]}],"source":["import numpy as np\n","\n","original_context = np.ones((347,45))\n","fake_targets = np.zeros(347)\n","\n","if (original_context[:,0] != fake_targets).sum() > 0:\n","    print(len(original_context))"]}],"metadata":{"interpreter":{"hash":"5c2ccdc665c0c79cd234ed900c895d5e07a64746ac62838ed47710c32430a718"},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
